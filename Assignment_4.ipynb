{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttourneux/Final-Project/blob/main/Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## questions: \n",
        "- do we need to use nn.Sequential? what are the advantages of using it? \n",
        "- are we supposed to assign self.decoder = nn.sequential(... all the layers)\n",
        "\n",
        "- am I implementing the VAE loss correctly?\n",
        "\n",
        "- what is the shape of z? "
      ],
      "metadata": {
        "id": "-gEP2wg7JBsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Author Information\n",
        "Name: \\<FILL HERE\\>\n",
        "\n",
        "B-Number: \\<FILL HERE\\>\n",
        "\n",
        "Email: \\<FILL HERE\\>"
      ],
      "metadata": {
        "id": "9XBxo9_JFRob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Instructions \n",
        "### Due December 5th, 11:59 PM.\n",
        "\n",
        "In the following assignment, you will be using variational autoencoders (VAEs) and generative adversarial networks (GANs) to generate new handwritten digits similar to the MNIST dataset.   \n",
        "\n",
        "\n",
        "Functions and cells that need to be implemented are marked with a bold **implement** keyword or clearly marked in the experiments section. \n",
        "\n",
        "The experiments section for each classifier also need to be implemented. You should follow the instructions above the cell. You may also add additional cells. "
      ],
      "metadata": {
        "id": "dwQubWAOFWgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Statements"
      ],
      "metadata": {
        "id": "66ci8GC9HMxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision.datasets as dset\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# for plotting\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['font.size'] = 16\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ],
      "metadata": {
        "id": "5m8H5miUHNDg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "Generative models are notoriously finicky with hyperparameters, and also require many training epochs. \n",
        "\n",
        "We will be working on the MNIST dataset, which is 60,000 training and 10,000 test images. Each picture contains a centered image of white digit on black background (0 through 9). \n",
        "\n",
        "To simplify our code here, we will use the PyTorch MNIST wrapper, which downloads and loads the MNIST dataset. See the [documentation](https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py) for more information about the interface. The default parameters will take 5,000 of the training examples and place them into a validation dataset. The data will be saved into a folder called `MNIST_data`. "
      ],
      "metadata": {
        "id": "-n8qpvuNGeLJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZo0wD82FIGo",
        "outputId": "062ac4fc-c362-46ad-d19f-55de92b6970c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Mount the drive\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''# Mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Run* the following the following cell to download the MNIST data set."
      ],
      "metadata": {
        "id": "jPogmJsmHI-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "print('download MNIST if not exist')\n",
        "\n",
        "mnist_train = dset.MNIST('./MNIST_data', train=True, download=True,\n",
        "                           transform=T.ToTensor())\n",
        "loader_train = DataLoader(mnist_train, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True, num_workers=2)\n",
        "\n",
        "\n",
        "imgs = loader_train.__iter__().next()[0].view(batch_size, 784)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcFbhFakHJN5",
        "outputId": "c5729b25-baa7-4035-81fe-0935b01d087b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download MNIST if not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Run* this cell to define the helper function to generate grids of images. "
      ],
      "metadata": {
        "id": "_jwQQ0dgHFQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images):\n",
        "    images = torch.reshape(\n",
        "        images, [images.shape[0], -1]\n",
        "    )  # images reshape to (batch_size, D)\n",
        "    sqrtn = int(math.ceil(math.sqrt(images.shape[0])))\n",
        "    sqrtimg = int(math.ceil(math.sqrt(images.shape[1])))\n",
        "\n",
        "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
        "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
        "    gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        ax = plt.subplot(gs[i])\n",
        "        plt.axis(\"off\")\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_aspect(\"equal\")\n",
        "        plt.imshow(img.reshape([sqrtimg, sqrtimg]))\n",
        "    return"
      ],
      "metadata": {
        "id": "mD5aqLmqHFbv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Run* the following cell to show an example of a grid of MNIST images."
      ],
      "metadata": {
        "id": "04CqbClrPNL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(imgs[:36])"
      ],
      "metadata": {
        "id": "evAMX2GrHl_W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "6641aca9-fb74-46b2-9f60-268fca81d0ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 36 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Part 1) Variational Autoencoder - 40 pts\n",
        "\n"
      ],
      "metadata": {
        "id": "p4r8iP4yHBSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE Encoder\n",
        "\n",
        "Now lets start building our fully-connected VAE network.\n",
        "Start with the encoder,\n",
        "which will take our images as input and pass them through a three Linear+ReLU layers.\n",
        "You will use the output of these layers to predict both the posterior mu and posterior log-variance using two separate linear layers (both shape (N,Z)). \n",
        "\n",
        "\n",
        "Use nn.Sequential to define the encoder, and separate Linear layers for the mu and logvar layers. In all of these layers, H will be a hidden dimension you set and will be the same across all encoder and decoder layers. Architecture for the encoder is described below:\n",
        "\n",
        " * `Flatten` (Hint: nn.Flatten)\n",
        " * Fully connected layer with input size 784 (`input_size`) and output size H\n",
        " * `ReLU`\n",
        " * Fully connected layer with input_size H and output size H\n",
        " * `ReLU`\n",
        " * Fully connected layer with input_size H and output size H\n",
        " * `ReLU`\n",
        "\n",
        "Complete the `Encoder` class by completing the `self.__init__` and `self.forward` methods.  "
      ],
      "metadata": {
        "id": "4dajulh-7TkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # ==== BEGIN SOLUTION CODE ====   \n",
        "        self.input_size = input_size \n",
        "        self.hidden_dim = hidden_dim     \n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(784,self.hidden_dim) \n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            self.flatten,\n",
        "            self.fc1,self.ReLU,\n",
        "            self.fc2, self.ReLU,\n",
        "            self.fc3, self.ReLU\n",
        "        )\n",
        "\n",
        "        \n",
        "\n",
        "        # ==== END SOLUTION CODE ====\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Use the self._encoder to encode the image to the hidden dimension\n",
        "        \n",
        "        Inputs:\n",
        "        - x: Tensor of shape (N, 1, H, W ) where H * W = input_size\n",
        "\n",
        "        Returns:\n",
        "        - h: Tensor of shape (N , hidden_dim)\n",
        "        '''\n",
        "        #h = None\n",
        "        # ==== BEGIN SOLUTION CODE ====\n",
        "        '''x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.ReLU(x) \n",
        "        x = self.fc2(x) \n",
        "        x = self.ReLU(x) \n",
        "        x = self.fc3(x) \n",
        "        h = x '''\n",
        "        h = self.encoder(x)\n",
        "\n",
        "        # ==== END SOLUTION CODE ====\n",
        "        return h"
      ],
      "metadata": {
        "id": "GUat8css7T5Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Implement* a sanity check in the following cell to run a `(32, 1, 28, 28)` random tensor through an encoder initialized with size `786` and `96` dim hidden dimension. The output should be `(32, 96)`. "
      ],
      "metadata": {
        "id": "ooHtzwWLbUjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BEGIN SOLUTION CODE ====\n",
        "encoder = Encoder(786,96)\n",
        "rtensor = torch.rand((32,1,28,28))\n",
        "\n",
        "ans = encoder.forward(rtensor)\n",
        "ans.shape\n",
        "\n",
        "# ==== END SOLUTION CODE ===="
      ],
      "metadata": {
        "id": "PRCA7vc_bVgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8c6a58-f6c9-4420-f127-5807e13f4c31"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 96])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE Decoder\n",
        "\n",
        "**Implement** the Decoder, which will take the latent space representation and generate a reconstructed image. The architecture is as follows: \n",
        "\n",
        " * Fully connected layer with input size as the latent size (Z) and output size hidden dim (H)\n",
        " * `ReLU`\n",
        " * Fully connected layer with input_size H and output size H\n",
        " * `ReLU`\n",
        " * Fully connected layer with input_size H and output size H\n",
        " * `ReLU`\n",
        " * Fully connected layer with input_size H and output size (`output_size` = 784)\n",
        " * `Sigmoid`\n",
        " * `Unflatten` (nn.Unflatten: shape=(1,28,28))\n",
        "\n",
        "Use `nn.Sequential` to define the architecture in `self._decoder`. \n",
        "\n",
        "Complete the `Decoder` class by completing the `self.__init__` and `self.forward` methods.  \n"
      ],
      "metadata": {
        "id": "RkMN6F4g651k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_size, hidden_dim, height=28, width=28):\n",
        "        super().__init__()\n",
        "        self._decoder = None\n",
        "        self.latent_size = latent_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.output_size = height * width\n",
        "\n",
        "        # ==== BEGIN SOLUTION CODE ====\n",
        "        self.ReLU = nn.ReLU()\n",
        "        \n",
        "        self.fc1 = nn.Linear(self.latent_size,self.hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim) \n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim) \n",
        "        self.fc4 = nn.Linear(hidden_dim, self.output_size)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.unflatten = nn.Unflatten(1, (1,self.width,self.height))\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            self.fc1,self.ReLU,\n",
        "\n",
        "            self.fc2,self.ReLU,\n",
        "\n",
        "            self.fc3,self.ReLU,\n",
        "\n",
        "            self.fc4,self.sigmoid,\n",
        "\n",
        "            self.unflatten,\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # ==== END SOLUTION CODE ====\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Use the self._decoder to reconstruct the image\n",
        "        \n",
        "        Inputs:\n",
        "        - Z: Tensor of shape (N, Z) giving the latent vector\n",
        "\n",
        "        Returns:\n",
        "        - x_reconstructed: Reconstruced input data of shape (N,1,H,W)\n",
        "          whete, H is self.height, and W is self.width\n",
        "        \"\"\"\n",
        "        x_reconstructed = None\n",
        "\n",
        "        # ==== BEGIN SOLUTION CODE ====\n",
        "        '''\n",
        "        x = self.fc1(z)\n",
        "        x = self.ReLU(x) \n",
        "\n",
        "\n",
        "        x = self.fc2(x) \n",
        "        x = self.ReLU(x)\n",
        "\n",
        "        x = self.fc3(x) \n",
        "        x = self.ReLU(x)\n",
        "\n",
        "        x = self.fc4(x) \n",
        "        x = self.sigmoid(x) \n",
        "\n",
        "        x_reconstructed = self.unflatten(x)'''\n",
        "\n",
        "\n",
        "        x_reconstructed = self.decoder(z)\n",
        "\n",
        "\n",
        "        # ==== END SOLUTION CODE ====\n",
        "        return x_reconstructed    \n"
      ],
      "metadata": {
        "id": "0H8urU548HSK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Implement* a sanity check in the following cell to run a `(16, 32)` random tensor through a decoder initialized with a `32` and `96` dim latent space and hidden dimension respectively. The output should be `(16, 1, 28, 28)`"
      ],
      "metadata": {
        "id": "R3aIwq59bgLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BEGIN SOLUTION CODE ====\n",
        "        \n",
        "\n",
        "decoder = Decoder(32,96)\n",
        "rtensor = torch.rand((16,32))\n",
        "\n",
        "ans = decoder.forward(rtensor)\n",
        "ans.shape\n",
        "# ==== END SOLUTION CODE ===="
      ],
      "metadata": {
        "id": "VdV2-o4lbgWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b79d1a28-0390-4911-d49d-3007135958c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reparameterization Trick\n",
        "\n",
        "Apply a reparametrization trick in order to estimate the posterior $z$ during our forward pass, given the $\\mu$ and $log(\\sigma^2)$ estimated by the encoder.\n",
        "\n",
        "In order to be able to backprop through the probabilistic latent space, we sample initial random data $\\epsilon$ from a fixed distrubtion, and compute $z$ as a function of ($\\epsilon$, $\\log(\\sigma^2)$, $\\mu$). Specifically:\n",
        "\n",
        "$z = \\mu + \\sigma\\epsilon$\n",
        "\n",
        "We can easily find the partial derivatives w.r.t $\\mu$ and $\\sigma^2$ and backpropagate through $z$. If $\\epsilon = \\mathcal{N} (0,1)$, then it's easy to verify that the result of our forward pass calculation will be a distribution centered at $\\mu$ with variance $\\sigma^2$.\n",
        "\n",
        "Not that the encoder outputs $\\log(\\sigma^2))$. So you will need to transform the output first before applying the reparameterization trick. \n"
      ],
      "metadata": {
        "id": "_CQ5zLqr8Hu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the reparametrize function."
      ],
      "metadata": {
        "id": "34ez_hoZR3_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reparameterize(mu, logvar):\n",
        "    \"\"\"\n",
        "    Differentiably sample random Gaussian data with specified mean and variance\n",
        "    using the reparameterization trick.\n",
        "\n",
        "    We want to sample a random number z from a Gaussian distribution with\n",
        "    mean mu and standard deviation sigma, such that we can backpropagate from the\n",
        "    z back to mu and sigma. We can achieve this by first sampling a random value\n",
        "    epsilon from a standard Gaussian distribution with zero mean and unit variance,\n",
        "    then setting z = sigma * epsilon + mu.\n",
        "\n",
        "    For more stable training when integrating this function into a neural network,\n",
        "    it helps to pass this function the log of the variance of the distribution from\n",
        "    which to sample, rather than specifying the standard deviation directly.\n",
        "\n",
        "    Inputs:\n",
        "    - mu: Tensor of shape (N, Z) giving means\n",
        "    - logvar: Tensor of shape (N, Z) giving log-variances\n",
        "\n",
        "    Returns:\n",
        "    - z: Estimated latent vectors, where z[i, j] is a random value sampled from a\n",
        "      Gaussian with mean mu[i, j] and log-variance logvar[i, j].\n",
        "    \"\"\"\n",
        "\n",
        "    # ==== BEGIN SOLUTION CODE ====\n",
        "    N,Z = mu.shape\n",
        "    epsilon = torch.normal(torch.zeros((N,Z)),torch.ones(N,Z)).to(device)\n",
        "    sigma = torch.exp(logvar).to(device)\n",
        "\n",
        "    mu = mu.to(device)\n",
        "\n",
        "    z = sigma*epsilon + mu\n",
        "    assert(z.shape == (N,Z))\n",
        "    # ==== END SOLUTION CODE ====\n",
        "    \n",
        "    return z"
      ],
      "metadata": {
        "id": "sHpIwF8LoDyV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE Model\n",
        "\n"
      ],
      "metadata": {
        "id": "30ZTlVuOoDNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the following cell and complete the implementation of the VAE class. You will need to complete the `self.__init__`, and `self.forward` methods. \n",
        "\n",
        "To complete the `self.__init__` method: \n",
        "- Define self.encoder which should be a network that\n",
        "inputs a batch of input images of shape (N, 1, H, W) into a batch of hidden features of shape (N, H_d). Use the `Encoder` class you defined before. \n",
        "\n",
        "- Set up self.mu_layer and self.logvar_layer to be a pair of linear layers that map the hidden features into estimates of the mean and log-variance of the posterior over the latent vectors; the mean and log-variance estimates will both be tensors of shape (N, Z). \n",
        "Note that we are calling this the 'logvar' layer because we'll use the log-variance (instead of variance or standard deviation) to stabilize training. This will specifically matter more when you compute reparametrization and the loss function. \n",
        "\n",
        "- Define self.decoder which should be a network that inputs a batch of latent vectors of shape (N, Z) and outputs a tensor of estimated images of shape (N, 1, H, W). Use the `Decoder` class you defined before. \n",
        "\n",
        "To complete the `self.forward` method:\n",
        "\n",
        "- Get the hidden dimension vector by calling passing the input through the encoder\n",
        "\n",
        "- Use `self.mu_layer` and `self.logvag_layer` to obtain the posterior mu and logvar\n",
        "\n",
        "- Use the `reparameterize` function to compute the latent vector z\n",
        "\n",
        "- Use the decoder to reconstruct the input \n",
        "\n",
        "- Return the reconstructed input, the posterior mean, and the posterior logvar. "
      ],
      "metadata": {
        "id": "jBeyDYjKKUjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, height, width, hidden_dim=96, latent_size=16):\n",
        "        super(VAE, self).__init__()\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.input_size = height * width  # H*W\n",
        "        self.latent_size = latent_size  # Z\n",
        "        self.hidden_dim = hidden_dim  \n",
        "        self.encoder = None\n",
        "        self.mu_layer = None\n",
        "        self.logvar_layer = None\n",
        "        self.decoder = None\n",
        "\n",
        "        # ==== BEGIN SOLUTION CODE ====\n",
        "\n",
        "        self.encoder = Encoder(height*width, hidden_dim=hidden_dim)\n",
        "\n",
        "        self.decoder = Decoder(latent_size = self.latent_size,hidden_dim=hidden_dim)\n",
        "\n",
        "        self.mu_layer =nn.Linear(hidden_dim,latent_size)\n",
        "\n",
        "        self.logvar_layer = nn.Linear(hidden_dim,latent_size)\n",
        "        # ==== END SOLUTION CODE ====\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs forward pass through VAE model by passing image through\n",
        "        encoder, reparametrize trick, and decoder models\n",
        "\n",
        "        Inputs:\n",
        "        - x: Batch of input images of shape (N, 1, H, W)\n",
        "\n",
        "        Returns:\n",
        "        - x_hat: Reconstruced input data of shape (N,1,H,W)\n",
        "        - mu: Matrix representing estimated posterior mu (N, Z), with Z latent\n",
        "          space dimension\n",
        "        - logvar: Matrix representing estimataed variance in log-space (N, Z),\n",
        "          with Z latent space dimension\n",
        "        \"\"\"\n",
        "        x_hat = None\n",
        "        mu = None\n",
        "        logvar = None\n",
        "        # ==== BEGIN SOLUTION CODE ====\n",
        "        hd_vector = self.encoder(x)\n",
        "        mu = self.mu_layer(hd_vector)\n",
        "        logvar = self.logvar_layer(hd_vector)\n",
        "\n",
        "        z = reparameterize(mu, logvar)\n",
        "        assert(z.shape == (128,self.latent_size))\n",
        "        \n",
        "\n",
        "        x_hat = self.decoder(z)\n",
        "\n",
        "        N,one,H,W = x.shape\n",
        "        assert(x_hat.shape == (N,1,H,W))\n",
        "        assert(mu.shape == (N,self.latent_size))\n",
        "        assert(logvar.shape == (N,self.latent_size) )\n",
        "        # ==== END SOLUTION CODE ====\n",
        "        return x_hat, mu, logvar"
      ],
      "metadata": {
        "id": "MNJgTfGR7tJ7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE Loss\n",
        "\n",
        "Before we're able to train our final model, we'll need to define our loss function. As seen below, the loss function for VAEs contains two terms: A reconstruction loss term (left) and KL divergence term (right). \n",
        "\n",
        "$$-E_{Z~q_{\\phi}(z|x)}[log p_{\\theta}(x|z)] + D_{KL}(q_{\\phi}(z|x), p(z)))$$\n",
        "\n",
        "Note that this is the negative of the variational lowerbound shown in lecture--this ensures that when we are minimizing this loss term, we're maximizing the variational lowerbound. The reconstruction loss term can be computed by simply using the binary cross entropy loss between the original input pixels and the output pixels of our decoder (Hint: `nn.functional.binary_cross_entropy` but choose the correct reduction). The KL divergence term works to force the latent space distribution to be close to a prior distribution (we're using a standard normal gaussian as our prior).\n",
        "\n",
        "To help you out, we've derived an unvectorized form of the KL divergence term for you.\n",
        "Suppose that $q_\\phi(z|x)$ is a $Z$-dimensional diagonal Gaussian with mean $\\mu_{z|x}$ of shape $(Z,)$ and standard deviation $\\sigma_{z|x}$ of shape $(Z,)$, and that $p(z)$ is a $Z$-dimensional Gaussian with zero mean and unit variance. Then we can write the KL divergence term as:\n",
        "\n",
        "$$D_{KL}(q_{\\phi}(z|x), p(z))) = -\\frac{1}{2} \\sum_{j=1}^{Z} (1 + log(\\sigma_{z|x}^2)_{j} - (\\mu_{z|x})^2_{j} - (\\sigma_{z|x})^2_{j})$$\n",
        "\n",
        "**Implement** this loss that also operates on minibatches.\n",
        "You should average the loss across samples in the minibatch.\n",
        "\n"
      ],
      "metadata": {
        "id": "IxCnOuvOoEDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def VAE_Loss(x, x_reconstructed, mu, logvar):\n",
        "  loss = None\n",
        "  # ==== BEGIN SOLUTION CODE ====\n",
        "\n",
        "  reconstruction_loss = nn.functional.binary_cross_entropy(x_reconstructed,x)\n",
        "\n",
        "  N,Z = mu.shape\n",
        "  losses = []\n",
        "  '''for sample in range(N): \n",
        "    total = 0 \n",
        "    for j in range(Z): \n",
        "      total+= ( (1+logvar[sample,j]) - (mu[sample,j])**2 - torch.exp(logvar)[sample,j]**2 )## we might be able to do this in a vectorized fashion \n",
        "    total = -total/2#float(total/2)\n",
        "    #print('total',total)\n",
        "\n",
        "    losses.append(total)\n",
        "  #print('losses',losses)\n",
        "  loss = (sum(losses)/len(losses) ) + reconstruction_loss'''\n",
        "  #print('logvar',logvar)\n",
        "  #print('mu',mu)\n",
        "\n",
        "  d_KL = torch.ones((N,Z)).to(device) + logvar - mu**2 - torch.exp(logvar)**2\n",
        "  d_KL_loss = -.5*sum(d_KL.flatten())/len(d_KL.flatten())\n",
        "  \n",
        "  ##print('d_KL_loss',d_KL_loss)\n",
        "  #print('reconstruction_loss', reconstruction_loss)\n",
        "  # ==== END SOLUTION CODE ====\n",
        "  return d_KL_loss + reconstruction_loss"
      ],
      "metadata": {
        "id": "WZnhEd8_5Xms"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def VAE_Loss(x, x_reconstructed, mu, logvar):\n",
        "  loss = None\n",
        "  # ==== BEGIN SOLUTION CODE ====\n",
        "\n",
        "  reconstruction_loss = nn.functional.binary_cross_entropy(x,x_reconstructed)\n",
        "\n",
        "  N,Z = mu.shape\n",
        "  losses = []\n",
        "  for sample in range(N): \n",
        "    total = 0 \n",
        "    for j in range(Z): \n",
        "      total+= ( (1+logvar[sample,j]) - (mu[sample,j])**2 - torch.exp(logvar)[sample,j]**2 )## we might be able to do this in a vectorized fashion \n",
        "    total = -total/2#float(total/2)\n",
        "    #print('total',total)\n",
        "\n",
        "    losses.append(total)\n",
        "  #print('losses',losses)\n",
        "  loss = (sum(losses)/len(losses) ) + reconstruction_loss\n",
        "  # ==== END SOLUTION CODE ====\n",
        "  return loss'''"
      ],
      "metadata": {
        "id": "KYF442FC8N_n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE Training\n",
        "\n",
        "In the following cell, define a VAE and train on the training set. \n",
        "\n",
        "The VAE training algorithm is as follows:\n",
        "\n",
        "1. Instantiate a `VAE` \n",
        "    - Suggested: hidden_dim = 96, latent_dim = 16\n",
        "2. Define a vae optimizer\n",
        "    - Suggested: ADAM with lr .001\n",
        "3. For each batch of data;\n",
        "    - Pass the input through the VAE to get the reconstructed input, the mean and log-var approximate of the posterior\n",
        "    - Calculate the loss using the `VAE_Loss` \n",
        "    - Zero the gradients, propagate the loss backwards, and update the  weights using the optimizer\n",
        "\n",
        "4. Repeat 3, `num_epoch` times\n",
        "    - Suggested: 10 times. \n",
        "\n",
        "*Following the suggested hyper-parameters, training takes approximately 2 minutes.* "
      ],
      "metadata": {
        "id": "3NK9F82a8OXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "qsTe3JUDm2Rv",
        "outputId": "fe808d41-aea2-4435-8a3d-6f110a0ea3a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''mnist_train, mnist_val = torch.utils.data.random_split(training_mnist, lengths =[int(len(training_mnist)*.80),int(len(training_mnist)*.2)])\n",
        "\n",
        "train_loader = DataLoader(dataset = mnist_train, batch_size = BATCH_SIZE)\n",
        "val_loader = DataLoader(dataset = mnist_val, batch_size = BATCH_SIZE)'''"
      ],
      "metadata": {
        "id": "7jakOh2Bne-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3748b03-ccd3-48bd-e86f-c4badcb2eae6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mnist_train, mnist_val = torch.utils.data.random_split(training_mnist, lengths =[int(len(training_mnist)*.80),int(len(training_mnist)*.2)])\\n\\ntrain_loader = DataLoader(dataset = mnist_train, batch_size = BATCH_SIZE)\\nval_loader = DataLoader(dataset = mnist_val, batch_size = BATCH_SIZE)'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#kl_div_loss = torch.nn.functional.kl_div(input, target, )"
      ],
      "metadata": {
        "id": "xdM9ruNxoRsh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logvar.shape"
      ],
      "metadata": {
        "id": "WK_hOufTx2Nk",
        "outputId": "c2122430-71a0-499d-ee1c-bbb3a314cd61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7304\\3899894523.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'logvar' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BEGIN SOLUTION CODE ====\n",
        "model = VAE(28,28,hidden_dim=96,latent_size =16)\n",
        "model.to(device)\n",
        "#BATCH_SIZE = 16 ## this is already set in the dataLoader\n",
        "EPOCHS = 10\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = .001)\n",
        "\n",
        "loss_stats= []\n",
        "\n",
        "for epoch in tqdm(range(1,EPOCHS+1),disable = True):\n",
        "  train_epoch_loss = 0 \n",
        "  model.train()\n",
        "  for x_train_batch, y_train_batch in tqdm(loader_train, disable = 0):\n",
        "    #print()\n",
        "    #print('start batch')\n",
        "    x_train_batch, y_train_batch = x_train_batch.to(device), y_train_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    x_hat, mu, logvar = model(x_train_batch.float())\n",
        "    train_loss = VAE_Loss(x_train_batch, x_hat, mu, logvar)\n",
        "    #print()\n",
        "    #print(train_loss)\n",
        "    #train_loss = torch.nn.functional.kl_div(logvar + mu, x_hat )\n",
        "\n",
        "\n",
        "    #print()\n",
        "    #print(\"train_loss\",train_loss)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_epoch_loss += train_loss\n",
        "    #print('end batch')## the batches are starting and ending...\n",
        "\n",
        "  loss_stats.append(train_epoch_loss)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "# ==== END SOLUTION CODE ====\n",
        "##1.11 loss with second VAE"
      ],
      "metadata": {
        "id": "WLaczWut8Qvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5655261a-40d7-419c-80eb-a61a69255a1c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 468/468 [00:20<00:00, 22.94it/s]\n",
            "100%|| 468/468 [00:20<00:00, 22.68it/s]\n",
            "100%|| 468/468 [00:20<00:00, 22.44it/s]\n",
            "100%|| 468/468 [00:21<00:00, 22.15it/s]\n",
            "100%|| 468/468 [00:21<00:00, 22.10it/s]\n",
            "100%|| 468/468 [00:21<00:00, 21.68it/s]\n",
            "100%|| 468/468 [00:21<00:00, 21.41it/s]\n",
            "100%|| 468/468 [00:22<00:00, 21.20it/s]\n",
            "100%|| 468/468 [00:22<00:00, 21.07it/s]\n",
            "100%|| 468/468 [00:22<00:00, 21.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cpu_loss_stats =[]\n",
        "for i in loss_stats: \n",
        "  cpu_loss_stats.append(i.to('cpu').detach().numpy())"
      ],
      "metadata": {
        "id": "yI2ok97Y76Ai"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_stats"
      ],
      "metadata": {
        "id": "8fHg_lXJ8VEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cpu_loss_stats)"
      ],
      "metadata": {
        "id": "dCgfdYk1cDTm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "b3e5d1db-1d8e-41e1-a392-d7d0acba29d9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x29881078a48>]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Interpolation and Visualization\n"
      ],
      "metadata": {
        "id": "8DgkGJxk8RQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generated Image Visualization\n",
        "\n",
        "**Implement** the following cell to generate 36 random samples from your previously trained VAE model and visualize the outputs. "
      ],
      "metadata": {
        "id": "wBVQw2hRXAPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BEGIN SOLUTION CODE ====\n",
        "\n",
        "model()\n",
        "    \n",
        "# ==== END SOLUTION CODE ===="
      ],
      "metadata": {
        "id": "8g7jiyfUXAed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "d3262b4a-6c12-4e2f-f426-6ddc1d5e1bbd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5488\\1685884557.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# ==== BEGIN SOLUTION CODE ====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# ==== END SOLUTION CODE ====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'x'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent Interpolation\n",
        "\n",
        "**Implement** the following cell to perform interpolation in latent space. Generate random latent vectors $z_0$ and $z_1$, and linearly interpolate between them; run each interpolated vector through the trained decoder to produce an image.\n",
        "\n",
        "$$\n",
        "z_{\\mathrm{interpolate}} = r_1*z_1 + (1-r_2)*z_2\n",
        "$$\n",
        "\n",
        "Take 10 uniformly spaced samples for $r_1$ and $r_2$ in the range [0,1]. \n",
        "\n",
        "Each row of the figure should interpolates between two random vectors. For the most part the model should exhibit smooth transitions along each row, demonstrating that the model has learned something nontrivial about the underlying spatial structure of the digits it is modeling.\n",
        "\n",
        "You should use the helper plotting function. You should have a 10x10 grid of images. "
      ],
      "metadata": {
        "id": "Hx1bWmFcPiev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BEGIN SOLUTION CODE ====\n",
        "\n",
        "## I guess we should use the mu and logvar from the above cell (ie the output of our trained model) and reparameterize to get z_1 and z_2 ? \n",
        "\n",
        "# ==== END SOLUTION CODE ===="
      ],
      "metadata": {
        "id": "Fuc6BQ_o8YLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Part 2) Generative Autoencoder - 60 pts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u_hkjZG0HCNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator\n",
        "\n",
        "We will use a discriminator inspired by the TensorFlow MNIST classification tutorial, which is able to get above 99% accuracy on the MNIST dataset fairly quickly. \n",
        "* Reshape into image tensor (Use `nn.Unflatten`!)\n",
        "* Conv2D: 32 Filters, 5x5, Stride 1\n",
        "* Leaky ReLU(alpha=0.01)\n",
        "* Max Pool 2x2, Stride 2\n",
        "* Conv2D: 64 Filters, 5x5, Stride 1\n",
        "* Leaky ReLU(alpha=0.01)\n",
        "* Max Pool 2x2, Stride 2\n",
        "* Flatten\n",
        "* Fully Connected with output size 4 x 4 x 64\n",
        "* Leaky ReLU(alpha=0.01)\n",
        "* Fully Connected with output size 1"
      ],
      "metadata": {
        "id": "It-w4CH_K6ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # ==== BEGIN SOLUTION CODE ====\n",
        "    #self.discrimiator = nn.Sequential(\n",
        "    self.unflatten = nn.Unflatten(dim = 1, unflattened_size=(1,28,28))\n",
        "    self.cov1 =     nn.Conv2d(1,32,5)\n",
        "    self.leakyReLU =     nn.LeakyReLU(.01)\n",
        "    self.maxPool1 = nn.MaxPool2d(2, stride= 2 )\n",
        "    self.cov2 =     nn.Conv2d(32,64,5, stride = 2)\n",
        "    self.flatten =     nn.Flatten()\n",
        "    self.fc1  =  nn.Linear(256, 4*4*16)\n",
        "    #nn.LeakyReLU(.01)\n",
        "    self.fc2 =     nn.Linear(4*4*16,1)\n",
        "\n",
        "    #)\n",
        "    # ==== END SOLUTION CODE ====\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Performs forward pass through a discriminator model by passing image through\n",
        "    the neural network described above. The output is a single scalar. \n",
        "\n",
        "    Inputs:\n",
        "    - x: Batch of input images of shape (N, 1, H, W)\n",
        "\n",
        "    Returns:\n",
        "    - logits: PyTorch tensor of shape (N,1)\n",
        "    \"\"\"\n",
        "    logits = None\n",
        "    # ==== BEGIN SOLUTION CODE ====\n",
        "    x = self.unflatten(x)\n",
        "    x = self.cov1(x)\n",
        "    x = self.leakyReLU(x)\n",
        "    x = self.maxPool1(x)\n",
        "    x = self.cov2(x)\n",
        "    x = self.leakyReLU(x)\n",
        "    x = self.maxPool1(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.leakyReLU(x)\n",
        "    logits = self.fc2(x)\n",
        "    #print('x.shape',x.shape)\n",
        "    # ==== END SOLUTION CODE ====\n",
        "    return logits  "
      ],
      "metadata": {
        "id": "bqPzkX32JkMk"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Implement* a sanity check for the Discriminator. Input a random sample with shape `(32, 784)` to a discriminator and check if the output is `(32,1)`. "
      ],
      "metadata": {
        "id": "l9KvOcmwcWKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BEGIN SOLUTION CODE ====\n",
        "rtensor = torch.rand((32,784))\n",
        "check_discriminator = Discriminator()\n",
        "output = check_discriminator(rtensor)\n",
        "output.shape\n",
        "# ==== END SOLUTION CODE ===="
      ],
      "metadata": {
        "id": "k45H8iAwcWR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6167fa17-7e5f-4705-9f93-7fc03ee20e67"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator\n",
        "For the generator, we will copy the architecture exactly from the [InfoGAN paper](https://arxiv.org/pdf/1606.03657.pdf). See Appendix C.1 MNIST. \n",
        "\n",
        "* Fully connected with output size 1024\n",
        "* `ReLU`\n",
        "* BatchNorm\n",
        "* Fully connected with output size 7 x 7 x 128 \n",
        "* `ReLU`\n",
        "* BatchNorm\n",
        "* Reshape into Image Tensor of shape 7 x 7 x 128\n",
        "* ConvTranspose2D: 64 filters of 4x4, stride 2, 'same' padding (use `padding=1`)\n",
        "* `ReLU`\n",
        "* BatchNorm\n",
        "* ConvTranspose2D: 1 filter of 4x4, stride 2, 'same' padding (use `padding=1`)\n",
        "* `TanH`\n",
        "* Should have a 28 x 28 x 1 image, reshape back into 784 vector"
      ],
      "metadata": {
        "id": "5h_zIBQ-LQuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim):\n",
        "        super().__init__()\n",
        "        # ==== BEGIN SOLUTION CODE ====\n",
        "        self.fc1 = nn.Linear(noise_dim, 1024)## arg1 = 28*28\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.batchNorm1 = nn.BatchNorm1d(1024)\n",
        "        self.fc2 = nn.Linear(1024, 7*7*128)\n",
        "        #ReLU \n",
        "        self.batchNorm2 = nn.BatchNorm1d(7*7*128)\n",
        "\n",
        "        self.unflatten = nn.Unflatten(1,(128,7,7))## reshape? \n",
        "\n",
        "        self.covT1  = nn.ConvTranspose2d(128,64,4,stride=2,padding=1 )\n",
        "        self.batchNorm3 = nn.BatchNorm2d(64)#batchNorm\n",
        "        self.covT2  = nn.ConvTranspose2d(64,1,4,stride=2,padding=1 )\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.flatten = nn.Flatten()\n",
        " \n",
        "        # ==== END SOLUTION CODE ====\n",
        "\n",
        "\n",
        "    def forward(self, random_noise):\n",
        "        \"\"\"\n",
        "        Generates a batch of images from a random noise sample after passing \n",
        "        through the neural network. The output is a batch of images. \n",
        "\n",
        "        Inputs:\n",
        "        - x: Batch of random noise of shape (N, noise_dim)\n",
        "\n",
        "        Returns:\n",
        "        - fake_image: PyTorch tensor of shape (N, 1, 28, 28)\n",
        "        \"\"\"\n",
        "        fake_image = None\n",
        "        \n",
        "        # ==== BEGIN SOLUTION CODE ====\n",
        "        x = self.fc1(random_noise) \n",
        "        x = self.ReLU(x) \n",
        "        x = self.batchNorm1(x) \n",
        "\n",
        "        x = self.fc2(x) \n",
        "        x = self.ReLU(x)\n",
        "        x = self.batchNorm2(x)\n",
        "\n",
        "        x = self.unflatten(x)\n",
        "        #print('before covT1',x.shape)\n",
        "        x = self.covT1(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.batchNorm3(x)\n",
        "        #print('before covT2', x.shape)\n",
        "        x = self.covT2(x)\n",
        "        x = self.tanh(x)\n",
        "        #print('before flatten', x.shape)\n",
        "        fake_image = self.flatten(x)\n",
        "\n",
        "\n",
        "        # ==== END SOLUTION CODE ====\n",
        "        return fake_image"
      ],
      "metadata": {
        "id": "5SHZAH1ILRB7"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Implement* a sanity check for the Generator with random noise size `96`. Input a random sample with shape `(32, 96)` to a Generator and check if the output is `(32, 784)`. "
      ],
      "metadata": {
        "id": "yI5XQjgDcgaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BEGIN SOLUTION CODE ====\n",
        "rtensor = torch.rand((32, 96))\n",
        "check_generator = Generator(96)\n",
        "output = check_generator(rtensor)\n",
        "output.shape\n",
        "# ==== END SOLUTION CODE ===="
      ],
      "metadata": {
        "id": "UUJHBimocgnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a63a39e-1a98-4b63-c3b7-7c332512af2a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN Loss"
      ],
      "metadata": {
        "id": "qGy410jr8dhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminator Loss\n",
        "\n",
        "The discriminator loss is:\n",
        "$$ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
        "\n",
        "Instead of computing the expectation of $\\log D(G(z))$, $\\log D(x)$ and $\\log \\left(1-D(G(z))\\right)$, we will be averaging over elements of the minibatch, so make sure to combine the loss by averaging instead of summing.\n",
        "\n",
        "\n",
        "For the purpose of these equations, we assume that the output from the discriminator is a real number in the range $0 < D(x) < 1$ which results from squashing the raw score from the discriminator through a sigmoid function. However for a cleaner and more numerically stable implementation, we have not included the sigmoid in the discriminator architecture above -- instead we will implement the sigmoid as part of the loss function.\n",
        "\n",
        "**HINTS**: You can use the function [`torch.nn.functional.binary_cross_entropy_with_logits`](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits) to compute these losses in a numerically stable manner.\n",
        "\n",
        "Given a score $s\\in\\mathbb{R}$ and a label $y\\in\\{0, 1\\}$, the binary cross entropy loss (with logits) is defined as:\n",
        "\n",
        "$$ bce(s, y) = -y * \\log(\\sigma(s)) - (1 - y) * \\log(1 - \\sigma(s)) $$\n",
        "\n",
        "where $\\sigma(s)=1/(1+\\exp(-s))$ is the sigmoid function.\n",
        "\n",
        "You will also need to compute labels corresponding to real or fake and use the logit arguments to determine their size. Make sure you cast these labels to the correct data type and device using the `dtype` and `device` variables, for example:\n",
        "\n",
        "`true_labels = torch.ones(size, dtype=dtype, device=device)`\n",
        "\n",
        "**Implement** the `Discriminator_Loss` in the following cell.\n"
      ],
      "metadata": {
        "id": "otp3Mig8v-ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%debug"
      ],
      "metadata": {
        "id": "90SiA33ldYN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Discriminator_Loss(logits_real,\n",
        "                       logits_fake,\n",
        "                       device='cuda',\n",
        "                       dtype=torch.float):\n",
        "    \"\"\"\n",
        "    Computes the discriminator loss described above.\n",
        "\n",
        "    Inputs:\n",
        "    - logits_real: PyTorch Tensor of shape (N,) giving scores for the real data.\n",
        "    - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
        "    - device: String  giving device type (default: 'cuda')\n",
        "    - dtype: PyTorch type (default: torch.float)\n",
        "\n",
        "    Returns:\n",
        "    - loss: PyTorch Tensor containing (scalar) the loss for the discriminator.\n",
        "    \"\"\"\n",
        "    loss = None\n",
        "    # ==== BEGIN SOLUTION CODE ====\n",
        "    ## logits_real is just 0 or 1 which is whether the image is fake or now \n",
        "    ## logits_fake is a probability \n",
        "    fake_labels = torch.ones(logits_fake.size, dtype = dtype, device = device)\n",
        "    fake_loss = nn.functional.binary_cross_entropy_with_logits(logits_fake,fake_labels)## the reduction is already the mean\n",
        "\n",
        "    real_labels = torch.zeros(logits_real.shape, dtype = dtype, device = device)\n",
        "    real_loss = nn.functional.binary_cross_entropy_with_logits(logits_real,real_labels)\n",
        "    # ==== END SOLUTION CODE ====\n",
        "    return (real_loss + fake_loss)/2"
      ],
      "metadata": {
        "id": "zEsr_eQfv-io"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator Loss\n",
        "\n",
        "The generator loss is:\n",
        "$$\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
        "\n",
        "This can be accomplished by comparing the outputs of $D(G(z))$, the fake logits, to the real labels. This may seem a bit counter-intuitive, but remember, we wish to only use the first term in the bce loss with logits and igonre the $log(1-\\sigma(s))$ term. \n",
        "\n",
        "\n",
        "Instead of computing the expectation of $\\log D(G(z))$, we will be averaging over elements of the minibatch, so make sure to combine the loss by averaging instead of summing. \n",
        "\n",
        "Same as the discriminator loss, you can use the function [`torch.nn.functional.binary_cross_entropy_with_logits`](https://pytorch.org/docs/stable/nn.functional.html#binary-cross-entropy-with-logits) to compute these losses in a numerically stable manner.\n",
        "\n",
        "Given a score $s\\in\\mathbb{R}$ and a label $y\\in\\{0, 1\\}$, the binary cross entropy loss (with logits) is defined as:\n",
        "\n",
        "$$ bce(s, y) = -y * \\log(\\sigma(s)) - (1 - y) * \\log(1 - \\sigma(s)) $$\n",
        "\n",
        "where $\\sigma(s)=1/(1+\\exp(-s))$ is the sigmoid function. Note in this case, you only have fake (zero) labels. \n",
        "\n",
        "You will also need to compute labels corresponding to real or fake and use the logit arguments to determine their size. Make sure you cast these labels to the correct data type and device using the `dtype` and `device` variables, for example:\n",
        "\n",
        "`true_labels = torch.ones(size, dtype=dtype, device=device)`\n",
        "\n",
        "**Implement** the `Generator_Loss` in the following cell. "
      ],
      "metadata": {
        "id": "VOCFIqVlv8CW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Generator_Loss(logits_fake,\n",
        "                   device='cuda',\n",
        "                   dtype=torch.float):\n",
        "    \"\"\"\n",
        "    Computes the generator loss described above.\n",
        "\n",
        "    Inputs:\n",
        "    - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
        "    - device: String  giving device type (default: 'cuda')\n",
        "    - dtype: PyTorch type (default: torch.float)\n",
        "\n",
        "    Returns:\n",
        "    - loss: PyTorch Tensor containing the (scalar) loss for the generator.\n",
        "    \"\"\"\n",
        "    loss = None\n",
        "    # ==== BEGIN SOLUTION CODE ====\n",
        "    real_labels = torch.ones(logits_fake.shape, dtype = dtype, device = device )\n",
        "    loss = nn.functional.binary_cross_entropy_with_logits(logits_fake, real_labels)\n",
        "    \n",
        "    # ==== END SOLUTION CODE ====\n",
        "    return loss"
      ],
      "metadata": {
        "id": "dUMmThXR8d5y"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight initializer\n",
        "\n",
        "The GAN training is quite finicky, and seem to respond better to Xavier uniform initialization. \n",
        "\n",
        "**Run** the following cell to get define the `initialize_weights` function."
      ],
      "metadata": {
        "id": "wh_Ar4azYhfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    \"\"\"Initializes the weights of a torch.nn model using xavier initialization\"\"\"\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "metadata": {
        "id": "5PUDSkRqYhsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN Training"
      ],
      "metadata": {
        "id": "Ftl-tPPX8eek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have defined the `Generator`, `Discriminator`, `Generator_Loss`, and `Discriminator_Loss`. We are now ready to actually train our GAN! \n",
        "\n",
        "Unlike the VAE, in which the encoder and decoder are updated jointly, the GAN training alternates between updating generator and discriminator. Thus, we actually have two distinct models, with two different optimizers. \n",
        "\n",
        "The GAN training algorithm is as follows: \n",
        "\n",
        "1. Instantiate a `Generator` and `Discriminator`. Use the `apply` method of the generator and discriminator to use the `initialize_weights` function and intialize the weights with a Xavier Uniform distribution scheme.\n",
        "    - Suggested: latent dim = 96 or 74\n",
        "2. Define optimizers for the generator and discriminator\n",
        "    - Suggested: ADAM with lr=.001, beta1=0.5, beta2=0.999 \n",
        "3. For each batch of data:\n",
        "    - Zero the gradients in the discriminator\n",
        "    - Get the logits of the real data from the discriminator\n",
        "    - Generate random uniform noise of shape `(batch_size, latent_dim)`\n",
        "    - Generate fake data using the random noise and the generator\n",
        "    - Get the logits of the fake data from the discriminator\n",
        "    - Calculate the discriminator loss, propagate the gradients, update using the discriminator optimizer. Use the previously defined `Discriminator_Loss` to calculate the discriminator loss.\n",
        "    - Zero the gradients in the generator optimizer\n",
        "    - Generate random uniform noise of shape `(batch_size, latent_dim)`\n",
        "    - Generate fake data using the random noise and the generator\n",
        "    - Get the logits of the fake data from the discriminator\n",
        "    - Calculate the generator loss, propagate the gradients, update using the generator optimizer. Use the previously defined `Generator_Loss` to calculate the generator loss.\n",
        "\n",
        "4. Repeat 3 for `num_epochs` times\n",
        "    - Suggester: 10\n",
        "\n",
        "The random noise input to the generator should be in the range (-1, 1). \n",
        "\n",
        "*Hint: Modify the PyTorch function `torch.Rand`  to be in the correct range for random sampling.* \n",
        "\n",
        "**Implement** the training loop in the following cell. \n",
        "\n",
        "*Following the suggested hyper-parameters, training takes approximately 2 minutes on a GPU.* "
      ],
      "metadata": {
        "id": "8vFyOeoZyVnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define num_epochs and device in the next section. \n",
        "\n",
        "# ==== BEGIN SOLUTION CODE ====\n",
        "\n",
        "\n",
        "\n",
        "# ==== END SOLUTION CODE ====\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for x, _ in loader_train:\n",
        "        # Drop the last batch with irregular size to simplify the code\n",
        "        if len(x) != batch_size:\n",
        "          continue\n",
        "        \n",
        "        # Do not delete: numerical trick to increase distance between\n",
        "        # digit pixels and empty pixels\n",
        "        real_data = x.view(-1, 784).to(device)\n",
        "        real_data = 2*(real_data - 0.5)\n",
        "        # Do no delete\n",
        "\n",
        "        # ==== BEGIN SOLUTION CODE ====\n",
        "      \n",
        "\n",
        "        # ==== END SOLUTION CODE ===="
      ],
      "metadata": {
        "id": "mAwEtr5E8ewp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Interpolation and Visualization\n"
      ],
      "metadata": {
        "id": "6kcJettw8fBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generated Image Visualization\n",
        "\n",
        "**Implement** the following cell to generate 100 random samples from your previously trained Generator model and visualize the outputs. "
      ],
      "metadata": {
        "id": "J1RAaZ8EasJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BEGIN SOLUTION CODE ====\n",
        "\n",
        "# ==== END SOLUTION CODE ===="
      ],
      "metadata": {
        "id": "qmpAQeCEasXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent Interpolation\n",
        "\n",
        "**Implement** the following cell to perform interpolation in latent space. Generate random latent vectors $z_0$ and $z_1$, and linearly interpolate between them; run each interpolated vector through the trained generator to produce an image.\n",
        "\n",
        "$$\n",
        "z_{interpolate} = r_1*z_1 + (1-r_2)*z_2\n",
        "$$\n",
        "\n",
        "Take 10 uniformly spaced samples for $r_1$ and $r_2$ in the range [0,1]. \n",
        "\n",
        "Each row of the figure should interpolates between two random vectors. For the most part the model should exhibit smooth transitions along each row, demonstrating that the model has learned something nontrivial about the underlying spatial structure of the digits it is modeling.\n",
        "\n",
        "You should use the helper plotting function. You should have a 10x10 grid of images. "
      ],
      "metadata": {
        "id": "73y9C53tadRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BEGIN SOLUTION CODE ====\n",
        "\n",
        "# ==== END SOLUTION CODE ===="
      ],
      "metadata": {
        "id": "qxu6JFd_adbs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}